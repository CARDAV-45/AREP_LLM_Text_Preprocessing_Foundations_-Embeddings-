{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c5d01b7",
   "metadata": {},
   "source": [
    "# Embeddings y Procesamiento de Texto - LLMs desde Cero\n",
    "\n",
    "El objetivo de este notebook es entender cómo los modelos de lenguaje convierten texto en números, y luego esos números en vectores que capturan significado.\n",
    "\n",
    "Vamos a seguir paso a paso el pipeline: tokenización → fragmentos → embeddings → similaridad."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9579b0",
   "metadata": {},
   "source": [
    "## 1. Importar librerías y cargar el texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923e64b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import tiktoken\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Cargar el texto\n",
    "with open('the-verdict.txt', 'r', encoding='utf-8') as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "print(f\"Archivo cargado: {len(raw_text)} caracteres\")\n",
    "print(f\"Primeros 300 caracteres: {raw_text[:300]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946f2dd9",
   "metadata": {},
   "source": [
    "## 2. Tokenización: Convertir texto a números\n",
    "\n",
    "Los LLMs no entienden palabras, entienden tokens. tiktoken usa BPE (Byte-Pair Encoding), igual que GPT-2. Cada token es un número entre 0 y 50,256. Palabras comunes son un token, palabras raras pueden ser varios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d195e5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizar con GPT-2\n",
    "tokenizer = tiktoken.get_encoding('gpt2')\n",
    "token_ids = tokenizer.encode(raw_text)\n",
    "\n",
    "print(f\"Total de tokens: {len(token_ids)}\")\n",
    "print(f\"Vocab size (aprox): {tokenizer.n_vocab}\")\n",
    "print(f\"Primeros 20 tokens: {token_ids[:20]}\")\n",
    "\n",
    "# Ver qué dice el primer fragmento\n",
    "sample = token_ids[:10]\n",
    "decoded = tokenizer.decode(sample)\n",
    "print(f\"Primeros tokens decodificados: '{decoded}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d3fbd3",
   "metadata": {},
   "source": [
    "## 3. Ventana deslizante: Crear contextos\n",
    "\n",
    "Para entrenar un modelo, necesitamos pares (entrada, siguiente_token). Usamos una ventana deslizante que se mueve sobre el texto. El tamaño de la ventana (`max_length`) y cuánto se mueve (`stride`) controlan cuántos ejemplos generamos.\n",
    "\n",
    "Simulemos distintas configuraciones para ver el efecto del overlap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1828c068",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetLLM(Dataset):\n",
    "    \"\"\"Crea pares (input_tokens, target_token) con ventana deslizante.\"\"\"\n",
    "    def __init__(self, token_ids, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "        \n",
    "        # Ventana deslizante\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            target_chunk = token_ids[i + 1:i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk, dtype=torch.long))\n",
    "            self.target_ids.append(torch.tensor(target_chunk, dtype=torch.long))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]\n",
    "\n",
    "\n",
    "# Experimento: variar max_length y stride\n",
    "configs = [\n",
    "    (32, 32),\n",
    "    (32, 16),\n",
    "    (64, 16),\n",
    "    (128, 64),\n",
    "    (128, 32),\n",
    "    (256, 128)\n",
    "]\n",
    "\n",
    "print(\"Experimento: efecto del solapamiento en la cantidad de datos\\n\")\n",
    "for max_len, st in configs:\n",
    "    dataset = DatasetLLM(token_ids, max_len, st)\n",
    "    overlap_pct = ((max_len - st) / max_len) * 100 if max_len > st else 0\n",
    "    print(f\"max_length={max_len:3d}, stride={st:3d} → {len(dataset):4d} muestras | overlap: {overlap_pct:5.1f}%\")\n",
    "\n",
    "print(\"\\nObservación: stride pequeño = más solapamiento = más datos.\")\n",
    "print(\"Pero también más redundancia. Hay que encontrar el balance.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce82fef",
   "metadata": {},
   "source": [
    "## 4. Capa de Embeddings: Del número al vector\n",
    "\n",
    "Los embeddings son como una tabla de búsqueda gigante. Cada token (número) mapea a un vector de N dimensiones. En GPT-2, son vectores de 768 dimensiones.\n",
    "\n",
    "La pregunta clave: **¿Por qué estos vectores capturan significado?**\n",
    "\n",
    "Respuesta corta: Porque durante el entrenamiento, el modelo ajusta estos vectores mediante backprop para predecir el siguiente token. Tokens que aparecen en contextos similares terminan con vectores similares. Eso es lo que SÍ es significado: tokens que se usan de formas parecidas son similares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b4172d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear una capa de embeddings\n",
    "vocab_size = 50257  # Tamaño del vocabulario de GPT-2\n",
    "embedding_dim = 768  # Dimensión de los embeddings en GPT-2\n",
    "\n",
    "embedding_layer = torch.nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "print(f\"Capa de embeddings creada\")\n",
    "print(f\"  - Input: índices de tokens (0 a {vocab_size-1})\")\n",
    "print(f\"  - Output: vectores de {embedding_dim} dimensiones\")\n",
    "print(f\"  - Parámetros totales: {vocab_size * embedding_dim:,}\")\n",
    "\n",
    "# Generar embeddings para una muestra\n",
    "dataset_sample = DatasetLLM(token_ids, max_length=64, stride=32)\n",
    "loader = DataLoader(dataset_sample, batch_size=4)\n",
    "\n",
    "# Tomar un batch\n",
    "batch_inputs, batch_targets = next(iter(loader))\n",
    "print(f\"\\nBatch de entrada shape: {batch_inputs.shape}\")\n",
    "print(f\"  - {batch_inputs.shape[0]} ejemplos, cada uno con {batch_inputs.shape[1]} tokens\")\n",
    "\n",
    "# Generar embeddings\n",
    "embeddings = embedding_layer(batch_inputs)\n",
    "print(f\"\\nEmbeddings generados: {embeddings.shape}\")\n",
    "print(f\"  - {embeddings.shape[0]} ejemplos\")\n",
    "print(f\"  - {embeddings.shape[1]} posiciones (tokens en el contexto)\")\n",
    "print(f\"  - {embeddings.shape[2]} dimensiones (el vector)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aab3dfb",
   "metadata": {},
   "source": [
    "## 5. Similaridad: Los vectores sí capturan estructura\n",
    "\n",
    "Una forma de \"entender\" qué captura un embedding es mirar qué tan similares son según distance coseno. Dos tokens que generalmente aparecen en contextos parecidos tendrán embeddings parecidos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b9cce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tomar un embedding del batch anterior\n",
    "sample_embeddings = embeddings[0].detach().numpy()  # Shape: (64, 768)\n",
    "\n",
    "# Matriz de similaridad coseno entre los 64 tokens del contexto\n",
    "similarity_matrix = cosine_similarity(sample_embeddings)\n",
    "\n",
    "print(f\"Matriz de similaridad: {similarity_matrix.shape}\")\n",
    "print(f\"\")\n",
    "print(f\"Valores en la diagonal (token consigo mismo): ~1.0\")\n",
    "print(f\"Diagonal medio: {np.diag(similarity_matrix).mean():.3f}\")\n",
    "print(f\"\")\n",
    "print(f\"Valores típicos fuera de diagonal (tokens distintos):\")\n",
    "off_diagonal = similarity_matrix[np.triu_indices_from(similarity_matrix, k=1)]\n",
    "print(f\"  Min: {off_diagonal.min():.3f}\")\n",
    "print(f\"  Mean: {off_diagonal.mean():.3f}\")\n",
    "print(f\"  Max: {off_diagonal.max():.3f}\")\n",
    "print(f\"\")\n",
    "print(f\"Por ahora los embeddings son aleatorios (no entrenados),\")\n",
    "print(f\"pero la estructura está lista para cuando se entrene.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbaee3fb",
   "metadata": {},
   "source": [
    "## 6. Reflexión: Por qué importa esto\n",
    "\n",
    "Este flujo completo (tokenización → chunking → embeddings → comparación) es la base de:\n",
    "\n",
    "- **Recuperación semántica**: Encontrar contextos similares rápidamente (RAG)\n",
    "- **Razonamiento multi-paso**: Los agentes navegan usando similaridad vectorial\n",
    "- **Memoria**: Guardar y recuperar información según relevancia\n",
    "\n",
    "A partir de aquí, el siguiente paso son los **transformers**: agregar atención (\"¿cuál de estos tokens es importante?\") y más capas. Pero la base son los embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94fb7453",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resumen estadístico\n",
    "print(\"=\" * 60)\n",
    "print(\"RESUMEN DEL FLUJO\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "dataset_default = DatasetLLM(token_ids, max_length=128, stride=64)\n",
    "\n",
    "print(f\"\\n1. TEXTO ORIGINAL:\")\n",
    "print(f\"   - {len(raw_text)} caracteres\")\n",
    "print(f\"   - {len(token_ids)} tokens después de BPE\")\n",
    "\n",
    "print(f\"\\n2. TOKENIZACIÓN:\")\n",
    "print(f\"   - Vocabulario GPT-2: 50,257 tokens\")\n",
    "print(f\"   - Encoding: BPE (Byte-Pair Encoding)\")\n",
    "\n",
    "print(f\"\\n3. DATASET (con max_length=128, stride=64):\")\n",
    "print(f\"   - Muestras generadas: {len(dataset_default)}\")\n",
    "print(f\"   - Cada muestra: 128 tokens entrada + 128 tokens target\")\n",
    "print(f\"   - Solapamiento: 50%\")\n",
    "\n",
    "print(f\"\\n4. EMBEDDINGS:\")\n",
    "print(f\"   - Dimensión: 768 (como GPT-2)\")\n",
    "print(f\"   - Parámetros por token: 768\")\n",
    "print(f\"   - Parámetros totales capa: {vocab_size * embedding_dim:,}\")\n",
    "\n",
    "print(f\"\\n5. SIMILITUD:\")\n",
    "print(f\"   - Métrica: Coseno entre vectores\")\n",
    "print(f\"   - Rango: [-1, 1] típicamente [0, 1]\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"El código está listo para:\")\n",
    "print(\"  - Fine-tuning: entrenar estos embeddings con backprop\")\n",
    "print(\"  - Agentes: usar similaridad para recuperación y razonamiento\")\n",
    "print(\"  - Transformers: agregar capas de atención encima\")\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
